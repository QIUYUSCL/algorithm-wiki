
# 机器学习入门：线性回归 (Linear Regression)

::: tip 核心思想
试图找到一条直线（或超平面）$y = wx + b$，使得它能最好地拟合所有数据点。
:::

## 1. 假设函数 (Hypothesis)

我们定义假设函数 $h_\theta(x)$ 为：

$$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n$$

写成矩阵形式就是：

$$h_\theta(x) = \theta^T x$$

## 2. 损失函数 (Loss Function)

为了衡量模型预测得准不准，我们使用 **均方误差 (MSE)** 作为损失函数 $J(\theta)$：

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$$

其中：
- $m$ 是样本数量。
- $\frac{1}{2}$ 是为了方便求导时抵消系数。

## 3. 梯度下降 (Gradient Descent)

我们的目标是最小化 $J(\theta)$。通过不断沿着梯度的**反方向**更新参数 $\theta$：

$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)$$

其中 $\alpha$ 是**学习率 (Learning Rate)**，决定了下山的步子迈多大。

::: warning 调参坑点
- **学习率太大**：可能导致无法收敛，甚至发散。
- **学习率太小**：收敛速度极慢，训练时间过长。
:::